{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9eb8a38",
   "metadata": {},
   "source": [
    "# Thai Sentence Embedding Model Training\n",
    "\n",
    "This notebook fine-tunes a multilingual SentenceTransformer model for Thai sentence embeddings using the XNLI dataset and evaluates with the STSB benchmark.\n",
    "\n",
    "## Features\n",
    "- Loads and preprocesses Thai XNLI data for similarity learning\n",
    "- Uses `bert-base-multilingual-cased` as the base model\n",
    "- Evaluates with STSB (semantic textual similarity benchmark)\n",
    "- Saves the trained model for downstream Thai NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5843802",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets sentence-transformers python-dotenv\n",
    "\n",
    "# Import libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815415a",
   "metadata": {},
   "source": [
    "## Authentication Setup\n",
    "\n",
    "Enter your Hugging Face token to download datasets and models. You can get your token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face authentication\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ[\"HF_HUB_TOKEN\"] = hf_token\n",
    "\n",
    "# Set up cache directories\n",
    "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/content/hf_cache/datasets\"\n",
    "os.environ[\"HF_TRANSFORMERS_CACHE\"] = \"/content/hf_cache/transformers\"\n",
    "\n",
    "print(\"Authentication and cache setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614761b",
   "metadata": {},
   "source": [
    "## Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Set up logger to output info to stdout.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"Logging setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbca58",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_thai_data():\n",
    "    \"\"\"Load and preprocess Thai XNLI dataset as 0-1 similarity labels.\"\"\"\n",
    "    logger.info(\"Loading Thai XNLI dataset...\")\n",
    "    try:\n",
    "        xnli = load_dataset(\"xnli\", \"th\")\n",
    "        raw_train = xnli[\"train\"]\n",
    "\n",
    "        # Map: entailment -> 1.0; neutral & contradiction -> 0.0\n",
    "        mapping = {0: 1.0, 1: 0.0, 2: 0.0}\n",
    "\n",
    "        train_dataset = Dataset.from_dict(\n",
    "            {\n",
    "                \"sentence1\": raw_train[\"premise\"],\n",
    "                \"sentence2\": raw_train[\"hypothesis\"],\n",
    "                \"label\": [mapping[label] for label in raw_train[\"label\"]],\n",
    "            }\n",
    "        )\n",
    "        logger.info(f\"Loaded {len(train_dataset)} Thai training examples\")\n",
    "        return train_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading Thai data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load Thai training data\n",
    "train_dataset = load_thai_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data():\n",
    "    \"\"\"Load STS-B validation set normalized to 0-1 range for cosine-similarity evaluator.\"\"\"\n",
    "    logger.info(\"Loading STSB validation data...\")\n",
    "    try:\n",
    "        val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
    "        normalized_scores = [score / 5.0 for score in val_sts[\"label\"]]\n",
    "\n",
    "        evaluator = EmbeddingSimilarityEvaluator(\n",
    "            sentences1=val_sts[\"sentence1\"],\n",
    "            sentences2=val_sts[\"sentence2\"],\n",
    "            scores=normalized_scores,\n",
    "            main_similarity=\"cosine\",\n",
    "            name=\"stsb_eval\"\n",
    "        )\n",
    "        logger.info(f\"Created evaluator with {len(val_sts)} validation examples\")\n",
    "        return evaluator\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading validation data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load validation data\n",
    "evaluator = load_validation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26eed5",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_loss():\n",
    "    \"\"\"Create multilingual sentence transformer and cosine-similarity loss.\"\"\"\n",
    "    model_name = \"bert-base-multilingual-cased\"\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
    "    return embedding_model, train_loss\n",
    "\n",
    "# Create model and loss\n",
    "embedding_model, train_loss = create_model_and_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b079b",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_args():\n",
    "    \"\"\"Configure training arguments for fine-tuning the model.\"\"\"\n",
    "    return SentenceTransformerTrainingArguments(\n",
    "        output_dir=\"/content/thai_embedding_model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        fp16=True,\n",
    "        eval_steps=500,\n",
    "        logging_steps=100,\n",
    "        save_steps=1000,\n",
    "        load_best_model_at_end=False,  # Disable automatic best model loading\n",
    "        metric_for_best_model=\"eval_stsb_eval_spearman_cosine\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_drop_last=False,\n",
    "        learning_rate=2e-5,\n",
    "    )\n",
    "\n",
    "# Setup training arguments\n",
    "args = setup_training_args()\n",
    "logger.info(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc85f7",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=embedding_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "logger.info(\"==== Training started ====\")\n",
    "trainer.train()\n",
    "logger.info(\"==== Training finished ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde068ee",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model\n",
    "logger.info(\"Running final evaluation...\")\n",
    "result = evaluator(embedding_model)\n",
    "if isinstance(result, dict):\n",
    "    for metric, value in result.items():\n",
    "        if isinstance(value, float):\n",
    "            logger.info(f\"{metric}: {value:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"{metric}: {value}\")\n",
    "else:\n",
    "    logger.info(f\"Evaluation result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655051e5",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "final_model_path = \"/content/thai_sentence_transformer_final\"\n",
    "embedding_model.save(final_model_path)\n",
    "logger.info(f\"Model saved to: {final_model_path}\")\n",
    "\n",
    "# Create a zip file for easy download\n",
    "!zip -r /content/thai_sentence_transformer_final.zip /content/thai_sentence_transformer_final\n",
    "print(\"Model packaged for download at: /content/thai_sentence_transformer_final.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204164a3",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the trained model\n",
    "model = SentenceTransformer(final_model_path)\n",
    "\n",
    "# Test with Thai sentences\n",
    "thai_sentences = [\n",
    "    \"สวัสดีครับ\",\n",
    "    \"ขอบคุณมากครับ\",\n",
    "    \"ประโยคภาษาไทย\",\n",
    "    \"การเรียนรู้ของเครื่อง\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(thai_sentences)\n",
    "\n",
    "print(f\"Generated embeddings for {len(thai_sentences)} Thai sentences\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(\"\\nSample sentences:\")\n",
    "for i, sentence in enumerate(thai_sentences):\n",
    "    print(f\"{i+1}. {sentence}\")\n",
    "\n",
    "# Calculate similarity between first two sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "print(f\"\\nSimilarity between '{thai_sentences[0]}' and '{thai_sentences[1]}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994095e",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "You can download the trained model and training logs from the Files panel on the left. Look for:\n",
    "- `/content/thai_sentence_transformer_final.zip` - The complete trained model\n",
    "- `/content/thai_embedding_model/` - Training checkpoints and logs\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Download the model to your local machine\n",
    "2. Use it for Thai sentence similarity tasks\n",
    "3. Fine-tune further on your specific domain data\n",
    "4. Deploy for production use\n",
    "\n",
    "## License\n",
    "\n",
    "This project is for research and educational use. Please check the original model licenses before commercial use."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
